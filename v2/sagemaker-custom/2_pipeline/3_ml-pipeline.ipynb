{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating automated Machine Learning pipeline\n",
    "\n",
    "*** For this lab remember to use the Jupyter Notebook UI (not JupyterLab).** \n",
    "\n",
    ">We'll be using the [Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/) which currently [doesn't support for visualizations Jupyter Lab](https://github.com/aws/aws-step-functions-data-science-sdk-python/issues/29). If you want to run it in the JupyterLab environment, no problem, you just won't be able to visualize the Step Functions state machines inside the notebook (you'll have to go to the AWS console).\n",
    "\n",
    "\n",
    "Now that we have successfully developed the LightGBM model, we can proceed and create our ML pipeline. In a high-level, we want to create the following flow:\n",
    "\n",
    ">**Raw data** &rarr; **ETL** &rarr; **Train/Test datasets** &rarr; **Train our LightGBM model** &rarr; **Evaluate the F1 score automatically** &rarr; **Deploy model**\n",
    "\n",
    "This part of the workshop is composed of 4 parts:\n",
    "\n",
    "1. <a href=\"#setup\">Configuring the <strong>AWS Step Functions Data Science SDK</strong> and creating other necessary resources</a>\n",
    "2. <a href=\"#create_resources\">Create a <strong>AWS Glue ETL Job</strong> and <strong>AWS Lambda function</strong> for the Extract/Transform/Load step and model evaluation respectively and configure <strong>Amazon SageMaker</strong></a>\n",
    "3. <a href=\"#step_functions\">Building our Machine Learning pipeline with <strong>AWS Step Functions and the Data Science SDK</strong></a>\n",
    "4. <a href=\"#running_the_workflow\"><strong>Creating, running and testing the ML Workflow</strong></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"setup\">\n",
    "<h2>1. Setup</h2>\n",
    "</div>\n",
    "\n",
    "First, we'll need to **install and load all the required modules**. Then we'll create fine-grained IAM roles for the Lambda, Glue, and Step Functions resources that we will create. The IAM roles grant the services permissions within your AWS environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "from IPython import display\n",
    "import stepfunctions\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import s3_input\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import TrainingStep, ModelStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sm_arn = get_execution_role()\n",
    "id = uuid.uuid4().hex\n",
    "\n",
    "#Create a unique name for the AWS Glue job to be created. If you change the \n",
    "#default name, you may need to change the Step Functions execution role.\n",
    "glue_job_name = 'glue-iris-etl-{}'.format(id)\n",
    "\n",
    "#Create a unique name for the AWS Lambda function to be created. If you change\n",
    "#the default name, you may need to change the Step Functions execution role.\n",
    "function_name = 'query-training-status-{}'.format(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Notebook instance Role ARN:', sm_arn)\n",
    "stack_name='ml-pipeline-config'\n",
    "print('Stack Name:', stack_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To facilitate the creation of resources, we use some given CloudFormaion templates for creating IAM roles, policies and a Amazon S3 bucket:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws cloudformation create-stack --stack-name {stack_name} --template-body file://cfns/cfn-config-ml-pipeline.json --parameters ParameterKey=NotebookRoleArn,ParameterValue={sm_arn} --capabilities CAPABILITY_NAMED_IAM\n",
    "!aws cloudformation wait stack-create-complete --stack-name {stack_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save Cfn output variables to make it easier\n",
    "bucket = !aws cloudformation describe-stacks --stack-name {stack_name} --query \"Stacks[0].Outputs[?OutputKey=='S3PipelineBucket'].OutputValue\" --output text; \n",
    "bucket = bucket.s\n",
    "\n",
    "step_functions_role_arn = !aws cloudformation describe-stacks --stack-name {stack_name} --query \"Stacks[0].Outputs[?OutputKey=='StepFunctionsRoleArn'].OutputValue\" --output text; \n",
    "step_functions_role_arn = step_functions_role_arn.s\n",
    "\n",
    "glue_role_arn = !aws cloudformation describe-stacks --stack-name {stack_name} --query \"Stacks[0].Outputs[?OutputKey=='GlueRoleArn'].OutputValue\" --output text; \n",
    "glue_role_arn = glue_role_arn.s\n",
    "\n",
    "lambda_role_arn = !aws cloudformation describe-stacks --stack-name {stack_name} --query \"Stacks[0].Outputs[?OutputKey=='LambdaRoleArn'].OutputValue\" --output text; \n",
    "lambda_role_arn = lambda_role_arn.s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository_name = 'iris-model'\n",
    "role = get_execution_role()\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "\n",
    "print('ecr_repository_name:', ecr_repository_name)\n",
    "print('account_id:',account_id)\n",
    "print('region:',region)\n",
    "print('SageMaker notebook instance role:',role)\n",
    "print(\"Bucket Name:\", bucket)\n",
    "print(\"step_functions_role_arn:\", step_functions_role_arn)\n",
    "print(\"glue_role_arn:\",glue_role_arn)\n",
    "print(\"lambda_role_arn:\",lambda_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the Dataset**\n",
    "\n",
    "Now we are passed development, we want to save the data in our data lake, in a controled S3 bucket ([AWS Lake Formation can help us with this](https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html)). The bucket we created with CloudFormation above is going to be used for that.\n",
    "\n",
    "We'll save the data in the S3 bucket and we will organize it following this pattern:\n",
    "\n",
    "```\n",
    "/MY-BUCKET\n",
    "│ \n",
    "└── iris-classification (our project name)\n",
    "    ├── raw\n",
    "    ├── train \n",
    "    └── validation\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'iris-classification' #same name of previous S3 path for our iris-classification project\n",
    "\n",
    "raw_prefix = 'raw'\n",
    "train_prefix = 'train'\n",
    "test_prefix = 'test'\n",
    "\n",
    "source_data = 's3://{}/{}/{}/'.format(bucket, project_name, raw_prefix) \n",
    "train_data = 's3://{}/{}/{}/'.format(bucket, project_name, train_prefix)\n",
    "validation_data = 's3://{}/{}/{}/'.format(bucket, project_name, test_prefix)\n",
    "\n",
    "print('source_data:',source_data) \n",
    "print('train_data:',train_data) \n",
    "print('validation_data:',validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload the raw data to our S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "file_name = '../0_custom_train/lab/data/raw/iris.csv'\n",
    "object_name = '{}/{}/iris-raw.csv'.format(project_name, raw_prefix)\n",
    "s3.upload_file(file_name, bucket, object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![s3-ml-pipe](./media/s3-ml-pipe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"create_resources\">\n",
    "<h2>2. Creating the AWS Glue ETL job, AWS Lambda function and configuring Amazon SageMaker</h2>\n",
    "</div>\n",
    " \n",
    "In the following steps we'll create the Glue job and Lambda function that are called from the Step Functions workflow.\n",
    "\n",
    "#### Create the AWS Glue Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_path = 's3://{}/{}/code'.format(bucket, project_name)\n",
    "print('code_path:', code_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this simple PySpark code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./code/simple_glue_etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use the **Glue ETL Job with PySpark** just to split the data into a training and validation datasets. \n",
    "\n",
    "Obviously here it's an overkill, however the idea could be expanded to much larger datasets, allowing us to perform feature engineering at scale if wanted. Another option would be to use [Amazon SageMaker Processing Jobs](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#amazon-sagemaker-processing) if wanted (the idea would be similar with a cluster of multiple nodes processing the data in parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_script_location = S3Uploader.upload(local_path='./code/simple_glue_etl.py',\n",
    "                               desired_s3_uri=code_path,\n",
    "                               session=session)\n",
    "glue_client = boto3.client('glue')\n",
    "\n",
    "response = glue_client.create_job(\n",
    "    Name=glue_job_name,\n",
    "    Description='PySpark job to extract the data and split in to training and validation data sets',\n",
    "    Role=glue_role_arn,\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 2\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': glue_script_location,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python'\n",
    "    },\n",
    "    GlueVersion='2.0',\n",
    "    WorkerType='Standard',\n",
    "    NumberOfWorkers=2,\n",
    "    Timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the AWS Glue console and see the created ETL Job:\n",
    "\n",
    "[Click here!](https://console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=jobs)\n",
    "\n",
    "![glue-etl](./media/glue-etl.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the AWS Lambda Function\n",
    "\n",
    "Let's take a look at this simple Lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./code/query_training_status.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function queries the specified SageMaker training Job with `sm_client.describe_training_job(TrainingJobName=job_name)` and then filters the metrics with the name `validation:f1` (in `if metric['MetricName']=='validation:f1'`).\n",
    "\n",
    "The function returns the metric to Step Functions. Later, we'll create a state that checks the value of this F1 Score and approve or not the deployment automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_name = 'query_training_status.zip'\n",
    "lambda_source_code = './code/query_training_status.py'\n",
    "\n",
    "zf = zipfile.ZipFile(zip_name, mode='w')\n",
    "zf.write(lambda_source_code, arcname=lambda_source_code.split('/')[-1])\n",
    "zf.close()\n",
    "\n",
    "S3Uploader.upload(local_path=zip_name, \n",
    "                  desired_s3_uri=code_path,\n",
    "                  session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "response = lambda_client.create_function(\n",
    "    FunctionName=function_name,\n",
    "    Runtime='python3.7',\n",
    "    Role=lambda_role_arn,\n",
    "    Handler='query_training_status.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket,\n",
    "        'S3Key': '{}/code/{}'.format(project_name, zip_name)\n",
    "    },\n",
    "    Description='Queries a SageMaker training job and return the results.',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the AWS Lambda console and see the created ETL Job:\n",
    "\n",
    "[Click here!](https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions)\n",
    "\n",
    "![lambda-model-eval](./media/lambda-model-eval.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sm-estimator\">\n",
    "    <h4>Configure the SageMaker Estimator</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image_uri = f'{account_id}.dkr.ecr.us-east-1.amazonaws.com/iris-model:latest'\n",
    "sources = f's3://sagemaker-us-east-1-{account_id}/sagemaker-custom/code/sourcedir.tar.gz'\n",
    "entry_point = 'train.py'\n",
    "\n",
    "print('SAGEMAKER TRAINING JOB CONFIGURATIONS:\\n')\n",
    "print('Container image URI:\\n', container_image_uri)\n",
    "print('\\nSource tarball with training script:\\n', sources)\n",
    "print('\\nPython source file to be executed as the entry point to training:\\n', entry_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an estimator as in [the previous lab](../1_custom_inference/lab/2_inference-container.ipynb) (in the part `4. Testing the inference locally with our container using the SageMaker Python SDK`).\n",
    "\n",
    "Now we [set some Regex so that SageMaker can capture the F1 Score metric](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html#define-train-metrics-sdk) that our training container emits.\n",
    "\n",
    "> Remember that our training emits a log indicating the F1 score metric in the form `[F1 score] 0.94`, for example. The following Regex will capture the metric `'\\[F1 score\\] (.*?)$'`\n",
    "\n",
    "Finally, note that **we didn't set the hyperparameters. We will do that dynamically to trigger the ML pipeline with Step Functions**. We'll pass the configurations to the execution (so that we could trigger the pipeline multiple times and with different hyperparameters if we wanted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image_uri = container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/sagemaker-custom-lightgbm:latest'.format(account_id, region)\n",
    "print(container_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(container_image_uri,\n",
    "                                    sm_arn, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/training_output'.format(bucket, project_name),\n",
    "                                    enable_sagemaker_metrics=True,\n",
    "                                    metric_definitions=[\n",
    "                                        {'Name':'validation:loss', 'Regex': 'multi_logloss: (.*?)$'},\n",
    "                                        {'Name':'validation:f1', 'Regex':'\\[F1 score\\] (.*?)$'}\n",
    "                                    ]                                          \n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"step_functions\">\n",
    "<h2>3. Building our ML pipeline with AWS Step Functions and the Data Science SDK</h2>\n",
    "</div>\n",
    "\n",
    "\n",
    "### First of all, what is AWS Step Functions? Why are we using the AWS Step Functions Data Science SDK?\n",
    "\n",
    "In order to create a ML pipeline we can use [AWS Step Functions](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html) orchestrate the calls to multiple AWS services. [AWS Step Functions natively supports multiple AWS services like Amazon SageMaker, AWS Glue, Amazon EMR, AWS Lambda, and many others.](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-service-integrations.html)\n",
    "\n",
    "[Amazon States Language](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html), a JSON-based language, to define the state machines. \n",
    "\n",
    "A state machine is composed of **states** that can do work (**Task states**), determine which states to transition to next (**Choice states**), stop an execution with an error (**Fail states**), and so on. [More details here.](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-states.html) \n",
    "\n",
    "The basic idea is each a state **receives information from the previous state** (or the input that triggers the execution), performs some processing, and **it generates an output**, [appending data to the input, modifying the input data, filtering the data, etc.](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html)\n",
    "\n",
    "A simple example of a state machine would be:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"Comment\": \"A Hello World example of the Amazon States Language using Pass states\",\n",
    "  \"StartAt\": \"Hello\",\n",
    "  \"States\": {\n",
    "    \"Hello\": {\n",
    "      \"Type\": \"Pass\",\n",
    "      \"Result\": \"Hello\",\n",
    "      \"Next\": \"World\"\n",
    "    },\n",
    "    \"World\": {\n",
    "      \"Type\": \"Pass\",\n",
    "      \"Result\": \"World\",\n",
    "      \"End\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Yielding:**\n",
    "\n",
    "<img class=\"center\" src=\"./media/hello-world-state-machine.png\" alt=\"hello-world-state-machine\" width=150px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier, with the **AWS Data Science Workflows SDK** we can use several abstractions for creating state machines with AWS Step Functions directly. This way, we don't have to write big JSON, nor know specific details about the Amazon State Languages, nor know specific syntax for each service integration (e.g. request syntax for calling the [SageMaker CreateTrainingJob API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html))\n",
    "\n",
    "\n",
    "In this part of the lab you will create the following steps:\n",
    "\n",
    "* [**ETLStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/compute.html#stepfunctions.steps.compute.GlueStartJobRunStep) - Starts an AWS Glue job to extract the latest data from our source database and prepare our data.\n",
    "* [**TrainingStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep) - Creates the training step and passes the defined estimator.\n",
    "* [**ModelStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) - Creates a model in SageMaker using the artifacts created during the TrainingStep.\n",
    "* [**LambdaStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/compute.html#stepfunctions.steps.compute.LambdaStep) - Creates the task state step within our workflow that calls a Lambda function.\n",
    "* [**ChoiceStateStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/states.html#stepfunctions.steps.states.Choice) - Creates the choice state step within our workflow.\n",
    "* [**EndpointConfigStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointConfigStep) - Creates the endpoint config step to define the new configuration for our endpoint.\n",
    "* [**EndpointStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointStep) - Creates the endpoint step to update our model endpoint.\n",
    "* [**FailStateStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/states.html#stepfunctions.steps.states.Fail) - Creates fail state step within our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker expects unique names for each job, model and endpoint. \n",
    "# If these names are not unique the execution will fail.\n",
    "execution_input = ExecutionInput(schema={\n",
    "    'GlueJobName': str,\n",
    "    'TrainingJobName': str,\n",
    "    'Hyperparameters': {'sagemaker_program': str,\n",
    "                        'sagemaker_submit_directory': str,\n",
    "                        'num_leaves': str,\n",
    "                        'max_depth': str,\n",
    "                        'learning_rate': str,\n",
    "                        'random_state': str},\n",
    "    'ModelName': str,\n",
    "    'EndpointName': str,\n",
    "    'LambdaFunctionName': str,\n",
    "    'UpdateEndpoint': bool\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an ETL step with AWS Glue\n",
    "In the following cell, we create a Glue step thats runs an AWS Glue job. The Glue job extracts the latest data from our source database, removes unnecessary columns, splits the data in to training and validation sets, and saves the data to CSV format in S3. Glue is performing this extraction, transformation, and load (ETL) in a serverless fashion, so there are no compute resources to configure and manage. See the [GlueStartJobRunStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/compute.html#stepfunctions.steps.compute.GlueStartJobRunStep) Compute step in the AWS Step Functions Data Science SDK documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ETL CONFIGURATION:')\n",
    "print('source_data:',source_data)\n",
    "print('S3 destination URI:','s3a://{}/{}/'.format(bucket, project_name))\n",
    "print('train_prefix:', train_prefix)\n",
    "print('test_prefix:', test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_step = steps.GlueStartJobRunStep(\n",
    "    'Extract, Transform, Load',\n",
    "    parameters={\"JobName\": execution_input['GlueJobName'],\n",
    "                \"Arguments\":{\n",
    "                    '--S3_SOURCE': source_data,\n",
    "                    '--S3_DEST': 's3a://{}/{}/'.format(bucket, project_name),\n",
    "                    '--TRAIN_KEY': train_prefix + '/',\n",
    "                    '--TEST_KEY': test_prefix +'/'}\n",
    "               }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a SageMaker Training Step \n",
    "\n",
    "In the following cell, we create the training step and pass the estimator we defined above. See  [TrainingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAINING STEP CONFIGURATIONS:')\n",
    "print('train_data:',train_data)\n",
    "print('validation_data:', validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pass the hyperparameters with the placeholder `Hyperparameters`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = steps.TrainingStep(\n",
    "    'Model Training', \n",
    "    estimator=estimator,\n",
    "    hyperparameters=execution_input['Hyperparameters'],\n",
    "    data={\n",
    "        'train': s3_input(train_data, content_type='csv'),\n",
    "        'validation': s3_input(validation_data, content_type='csv')\n",
    "    },\n",
    "    job_name=execution_input['TrainingJobName'],\n",
    "    wait_for_completion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Model Step \n",
    "\n",
    "In the following cell, we define a model step that will create a model in Amazon SageMaker using the artifacts created during the TrainingStep. See  [ModelStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) in the AWS Step Functions Data Science SDK documentation to learn more.\n",
    "\n",
    "The model creation step typically follows the training step. The Step Functions SDK provides the [get_expected_model](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep.get_expected_model) method in the TrainingStep class to provide a reference for the trained model artifacts. Please note that this method is only useful when the ModelStep directly follows the TrainingStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = steps.ModelStep(\n",
    "    'Save Model',\n",
    "    model=training_step.get_expected_model(),\n",
    "    model_name=execution_input['ModelName'],\n",
    "    result_path='$.ModelStepResults'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Lambda Step\n",
    "In the following cell, we define a lambda step that will invoke the previously created lambda function as part of our Step Function workflow. See [LambdaStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/compute.html#stepfunctions.steps.compute.LambdaStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_step = steps.compute.LambdaStep(\n",
    "    'Query Training Results',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['LambdaFunctionName'],\n",
    "        'Payload':{\n",
    "            \"TrainingJobName.$\": '$.TrainingJobName'\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Choice State Step \n",
    "In the following cell, we create a choice step in order to build a dynamic workflow. This choice step branches based off of the results of our SageMaker training step: did the training job fail or should the model be saved and the endpoint be updated? We will add specfic rules to this choice step later on in section 8 of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy_step = steps.states.Choice(\n",
    "    'F1 score > 90%'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an Endpoint Configuration Step\n",
    "In the following cell we create an endpoint configuration step. See [EndpointConfigStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointConfigStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_configuration = sagemaker.model_monitor.data_capture_config.DataCaptureConfig(\n",
    "    enable_capture=True, \n",
    "    sampling_percentage=100, \n",
    "    destination_s3_uri='s3://{}/{}/endpoint_monitoring/'.format(bucket, project_name), \n",
    "    sagemaker_session=session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step = steps.EndpointConfigStep(\n",
    "    \"Create Model Endpoint Config\",\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    model_name=execution_input['ModelName'],\n",
    "    initial_instance_count=1,\n",
    "    variant_name='lgbmVariant',\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    data_capture_config=data_capture_configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the Model Endpoint Step\n",
    "In the following cell, we create the Endpoint step to deploy the new model as a managed API endpoint, updating an existing SageMaker endpoint if our choice state is sucessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_step = steps.EndpointStep(\n",
    "    'Update Model Endpoint',\n",
    "    endpoint_name=execution_input['EndpointName'],\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "#     update=execution_input['UpdateEndpoint']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Fail State Step\n",
    "In addition, we create a Fail step which proceeds from our choice state if the validation accuracy of our model is lower than the threshold we define. See [FailStateStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/states.html#stepfunctions.steps.states.Fail) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_step = steps.states.Fail(\n",
    "    'Model F1 score Too Low',\n",
    "    comment='Validation F1 score lower than threshold'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Rules to Choice State\n",
    "In the cells below, we add a threshold rule to our choice state. Therefore, if the **F1 score** of our model is below 0.90, we move to the Fail State. If the validation F1 score of our model is above 0.90, we move to the save model step with proceeding endpoint update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_rule = steps.choice_rule.ChoiceRule.NumericGreaterThan(variable=lambda_step.output()['Payload']['trainingMetrics'][0]['Value'], value=.9)\n",
    "\n",
    "check_accuracy_step.add_choice(rule=threshold_rule, next_step=endpoint_config_step)\n",
    "check_accuracy_step.default_choice(next_step=fail_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link all the Steps Together\n",
    "Finally, create your workflow definition by chaining all of the steps together that we've created. See [Chain](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.states.Chain) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step.next(endpoint_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_definition = steps.Chain([\n",
    "    etl_step,\n",
    "    training_step,\n",
    "    model_step,\n",
    "    lambda_step,\n",
    "    check_accuracy_step\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"running_the_workflow\">\n",
    "<h2>4. Creating, running and testing the ML Workflow</h2>\n",
    "</div>\n",
    "\n",
    "Create your workflow using the workflow definition above, and render the graph with [render_graph(...) method](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.render_graph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f'Iris-ML-Pipeline-{id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Workflow(\n",
    "    name=pipeline_name,\n",
    "    definition=workflow_definition,\n",
    "    role=step_functions_role_arn,\n",
    "    execution_input=execution_input\n",
    ")\n",
    "\n",
    "# For using existing workflow\n",
    "# workflow = Workflow.list_workflows(); workflow\n",
    "# workflow = Workflow.attach(workflow[0]['stateMachineArn']); workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update existing Step Functions State Machine\n",
    "# workflow.update(definition=workflow_definition,\n",
    "#                role=step_functions_role_arn)\n",
    "#\n",
    "# # View new definition\n",
    "# workflow.definition.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.render_graph(portrait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to CloudFormation (if desired)\n",
    "It is possible to simply export the State Machine above in a CloudFormation template, enabling teams to easily re-use and share pipelines (it's possible to create and publish your own products in [AWS Service Catalog](https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html) so that each team can easily use it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workflow.get_cloudformation_template())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View JSON of State Machine definition\n",
    "\n",
    "To see the AWS Step Functions state machine definition that the AWS Step Functions Data Science SDK created for us just use the `to_dict()` method (we can see how the SDK have made things easier for us and we didn't have to write the JSON):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.definition.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the workflow\n",
    "More details in documentation of AWS Step Functions: [create() method](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the state machines in the current region in this AWS account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Workflow.list_workflows(html=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's trigger the ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_job_name = f'iris-pipeline-{id}'\n",
    "model_name = f'Iris-{id}'\n",
    "endpoint_name = 'IrisMLPipeline'\n",
    "update_endpoint = False\n",
    "sources = f's3://sagemaker-us-east-1-{account_id}/sagemaker-custom/code/sourcedir.tar.gz',\n",
    "hyperparameters = {\n",
    "    \"sagemaker_program\": 'train.py',\n",
    "    \"sagemaker_submit_directory\": f's3://sagemaker-us-east-1-{account_id}/sagemaker-custom/code/sourcedir.tar.gz',\n",
    "    \"num_leaves\": '40',\n",
    "    \"max_depth\": '10',\n",
    "    \"learning_rate\": '0.11',\n",
    "    \"random_state\": '42'}\n",
    "\n",
    "print('INPUT CONFIGURATIONS:\\n')\n",
    "print('GlueJobName:\\n', glue_job_name)\n",
    "print('\\nHyperparameters\\n', hyperparameters)\n",
    "print('\\nTrainingJobName (each Sagemaker Job requires a unique number):\\n',training_job_name)\n",
    "print('\\nModelName (each SageMaker Model requires a unique name,):\\n', model_name)\n",
    "print('\\nEndpointName (each Endpoint requires a unique name):\\n', endpoint_name)\n",
    "print('\\nLambdaFunctionName:\\n', endpoint_name)\n",
    "print('\\nUpdateEndpoint:', update_endpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = workflow.execute(\n",
    "    inputs={\n",
    "        'GlueJobName': glue_job_name,\n",
    "        'TrainingJobName': training_job_name,\n",
    "        'Hyperparameters':  hyperparameters,\n",
    "        'ModelName': model_name,\n",
    "        'EndpointName': endpoint_name,\n",
    "        'LambdaFunctionName': function_name,\n",
    "        'UpdateEndpoint': update_endpoint\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render workflow progress with the [render_progress](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.render_progress). This generates a snapshot of the current state of your workflow as it executes. This is a static image therefore you must run the cell again to check progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If don't want to wait for the whole pipeline to finish, set to False\n",
    "wait = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just check current status if we don't want to check all flow\n",
    "if not wait:\n",
    "    execution.render_progress(portrait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the pipeline for the first time can take a few minutes because we are training and creating a SageMaker endpoint (spinning up instances). After that, updating an existing endpoint with newer models should be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = execution.describe().get('status')\n",
    "while status == 'RUNNING' and wait:\n",
    "    status = execution.describe().get('status')\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(10)\n",
    "    display.display(execution.render_progress(portrait=True))\n",
    "    \n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get to the `Model Training` state, go to the Amazon SageMaker console and see if everything went OK.\n",
    "\n",
    "**1. Look for the most recent training job:**\n",
    "\n",
    "[Click here](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs)\n",
    "\n",
    "![sm-pipe-training](./media/sm-pipe-training.png)\n",
    "\n",
    "**2. Scrolling down we can see that our metrics (regex) and hyperparameters were correctly set:**\n",
    "\n",
    "![sm-pipe-training-configs](./media/sm-pipe-training-configs.png)\n",
    "\n",
    "**3. Scrolling down again, click on the `View logs` and `View algorithm metrics` links:**\n",
    "\n",
    "![sm-pipe-training-monitor](./media/sm-pipe-training-monitor.png)\n",
    "\n",
    "**4. We see in CloudWatch our `Log stream` for the training. Click on it:**\n",
    "\n",
    "![sm-cw-logs](./media/sm-cw-logs.png)\n",
    "\n",
    "**5. We see the logs emited to the stdout of our training container. Go to the end of the logs.**\n",
    "\n",
    "We see the log `[F1 score] 0.9565217391304348`. This log should be captured in the metrics and SageMaker will associate it to our training.\n",
    "\n",
    "![sm-cw-logs-f1](./media/sm-cw-logs-f1.png)\n",
    "\n",
    "**6. We'll check if the F1 score metric was captured. Go to the other tab you opened in step 3. when you clicked on `View algorithm metrics`.**\n",
    "\n",
    "Select the both metrics captured by SageMaker with the <a href=\"#sm-estimator\">2 regular expressions that you configured in the estimator in the part 2 of this lab:</a>\n",
    "\n",
    "![sm-cw-metrics](./media/sm-cw-metrics.png)\n",
    "\n",
    "\n",
    "We see that SageMaker captured the final validation F1 score of 0.96 and validation loss of 0.08:\n",
    "\n",
    "![sm-cw-metrics-zoom](./media/sm-cw-metrics-zoom.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SageMaker captured the F1 score, the Lambda function that evaluates the model in the state `Query Training Results` was able to get it.\n",
    "\n",
    "In our case, the validation F1 score was over 0.96. Therefore, Step Functions will go to the [Choice state](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-choice-state.html) and approve the deployment automatically.\n",
    "\n",
    "We should see all states have passed and are green:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.display(execution.render_progress(portrait=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [list_executions](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.list_executions) to list all executions for a specific workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.list_executions(html=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [list_events](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.list_events) to list all events in the workflow execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution.list_events() #html=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait until our endpoint in service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "endpoint_status = 'Creating'\n",
    "while endpoint_status!='InService':\n",
    "    endpoint_status = sm_client.describe_endpoint(EndpointName='IrisMLPipeline')['EndpointStatus']\n",
    "    print(endpoint_status)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate some artificial traffic to our endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "with open('../0_custom_train/lab/data/test_no_label/iris_test_no_label.csv', 'r') as f:\n",
    "    for row in f:\n",
    "\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                      ContentType='text/csv', \n",
    "                                      Accept='text/csv',\n",
    "                                      Body=payload)\n",
    "        prediction = float(response['Body'].read().decode('utf-8').strip())\n",
    "        print('input:', row)\n",
    "        print('prediction:', prediction, '\\n')\n",
    "        time.sleep(1)\n",
    "\n",
    "endpoint_name = 'IrisMLPipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from stress import stress_button\n",
    "\n",
    "display(stress_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See traffic metrics and data captured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select your `IrisMLPipeline` endpoint in SageMaker:**\n",
    "\n",
    "![sm-endpoint.png](./media/sm-endpoint.png)\n",
    "\n",
    "**Click on the S3 bucket where the data captured from requests and predictions are stored:**\n",
    "\n",
    "![sm-endpoint-capture](./media/sm-endpoint-capture.png)\n",
    "\n",
    "**We should see objects being saved in the S3 bucket:**\n",
    "![sm-capture-s3](./media/sm-capture-s3.png)\n",
    "\n",
    "**We see that SageMaker has partitioned the bucket by endpoint (`IrisMLPipeline`), variant (`lgbVariant`), year, month, day hour:**\n",
    "\n",
    "![sm-capture-s3-dirs](./media/sm-capture-s3-dirs.png)\n",
    "\n",
    "**In your `IrisMLPipeline` endpoint in SageMaker, click on the `View invocation metrics` link. We will be redirected to the CloudWatch console:**\n",
    "![sm_ep_metrics](./media/sm_ep_metrics.png)\n",
    "\n",
    "**For CloudWatch, <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'Invocations~'EndpointName~'IrisMLPipeline~'VariantName~'lgbmVariant)~(~'.~'ModelLatency~'.~'.~'.~'.~(yAxis~'right~stat~'Average)))~view~'timeSeries~stacked~false~region~'us-east-1~stat~'Sum~period~60~start~'-PT5M~end~'P0D);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20IrisMLPipeline\">just click here.</a>**\n",
    "\n",
    "OR\n",
    "\n",
    "**Configure the Dashboard by selecting the metric `Invocations` and in the `Graphics metrics` choose Invocations and `Sum` in `Statistic`. Choose `Period` equals to 1 Minute and modify graphics in the top to show the window of 5 minutes.**\n",
    "\n",
    "![sm_cw_metrics](./media/sm_cw_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!! You have finished the workshop!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

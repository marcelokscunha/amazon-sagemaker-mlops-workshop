{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previous version of the SageMaker SDK\n",
    "import sys\n",
    "!{sys.executable} -m pip install sagemaker==1.72.0 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our custom LightGBM training container for SageMaker\n",
    "\n",
    "Now that we have successfully developed the LightGBM model, we can proceed and create our custom Docker container. We'll use the [sagemaker-training-toolkit library](https://github.com/aws/sagemaker-training-toolkit) to define script mode and framework containers (our custom LightGBM container).\n",
    "\n",
    "This part of the workshop is composed of 4 parts:\n",
    "\n",
    "1. <a href=\"#custom_training_container\">Create our <strong>custom Docker container that SageMaker will run for training</strong> in Script Mode and Framework mode</a>\n",
    "2. <a href=\"#training_toolkit\">Create a Python package to configure <strong>SageMaker Training toolkit</strong></a>\n",
    "3. <a href=\"#container_training_build\"><strong>Building</strong> our Docker image and <strong>pushing</strong> it to Amazon Elastic Container Registry</a>\n",
    "3. <a href=\"#testing_training\"><strong>Testing the training locally</strong> with our container using the SageMaker Python SDK</a>\n",
    "\n",
    "### First of all, what are the types of SageMaker containers? What is a SageMaker Framework container?\n",
    "\n",
    "Basically, SageMaker provides 3 types of containers (and APIs) that you can interact with:\n",
    "\n",
    "#### a) Basic Training Container (click on the three dots below for mode details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The bare minimum that is required for building a custom Docker container to run training in Amazon SageMaker (using pre-defined training logic and prepared to receive data in specific shape, e.g. target variable in the first column). \n",
    "\n",
    "<img src=\"./media/basic_training_container.jpg\" class=\"center\">\n",
    "\n",
    "When interacting with SageMaker with the basic training container, we use the Estimator API with the class ['sagemaker.estimator.Estimator(...)'](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Estimator) passing only the configurations like container image URI, hyperparamenters, etc. **We cannot pass custom code into SageMaker as it is already specified inside the Docker container.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Script Mode Container (click on the three dots below for mode details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "A custom container where **we can pass a user defined script that SageMaker will execute at training time** (sagemaker-training toolkit will load and execute the defined script as entry point). \n",
    "\n",
    "This training script can be passed to the container in a Python module (shown below in Script Mode Container on the left) or can be stored in Amazon S3 and passed to the container that will download it and run it as entry point (shown below in Script Mode Conatiner 2 on the right).\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"./media/script_mode_container.jpg\" style=\"height: 100%\"> </td>\n",
    "<td> <img src=\"./media/script_mode_container_2.jpg\" style=\"height: 100%\"> </td>\n",
    "</tr></table>\n",
    "\n",
    "We can use the Estimator API with the class ['sagemaker.estimator.Estimator(...)'](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Estimator) and pass the configurations  including `sagemaker_program` for the entry point Python module and `sagemaker_submit_directory` for URI of the tarball in S3 with the code used for training (this way SageMaker at training time knows where to download the code from and how to run it).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Framework Container (click on the three dots below for mode details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "A framework container is similar to a script-mode container, but in addition it loads a Python framework module that is used to configure the framework and then run the user-provided module. **The SageMaker Python SDK with Framework mode will create a tarball with local files and upload our code to S3. Then, in the training stage, SageMaker will load the script similarly to the Script Mode Container 2 above and run it for training.**\n",
    "\n",
    "<img src=\"./media/framework_container.jpg\" class=\"center\">\n",
    "\n",
    "We can use the Estimator API with the class ['sagemaker.estimator.Framework(...)'](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Framework) and pass the configurations  including `entry_point` for the entry point Python module and `source_dir` for the local directory with code.\n",
    "\n",
    "\n",
    "[More details about the containers and examples here.](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/custom-training-containers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Quick Recap**: [In the previous exercise](https://github.com/marcelokscunha/amazon-sagemaker-mlops-workshop/blob/master/lab/00_Warmup_Studio/xgboost_customer_churn_studio.ipynb) we used XGBoost in both \"basic training container\" and \"framework\" modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"custom_training_container\">\n",
    "<h2> 1. Creating the training container</h2>\n",
    "</div>\n",
    "\n",
    "We start by defining some variables like the current execution role, the ECR repository that we are going to use for pushing the custom Docker container and the default Amazon S3 bucket to be used by Amazon SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "ecr_repository_name = 'sagemaker-custom-lightgbm'\n",
    "role = get_execution_role()\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(account_id)\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket)\n",
    "print(ecr_repository_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the Dockerfile which defines the statements for building our custom script/framework container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ../docker/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "At high-level the Dockerfile specifies the following operations for building this container:\n",
    "1. Start from Ubuntu 16.04\n",
    "2. Define some variables to be used at build time to install Python 3\n",
    "3. Some handful libraries are installed with apt-get\n",
    "4. We then install Python 3 and create a symbolic link\n",
    "5. We copy a .tar.gz package named **custom_framework_training-1.0.0.tar.gz** in the WORKDIR\n",
    "6. We then install some Python libraries like Numpy, Pandas, Scikit-Learn, **LightGBM and our package we copied at the previous step**\n",
    "7. We set e few environment variables, including PYTHONUNBUFFERED which is used to avoid buffering Python standard output (useful for logging)\n",
    "8. Finally, we set the value of the environment variable `SAGEMAKER_TRAINING_MODULE` to a Python module in the training package we installed. \n",
    "\n",
    "> **Under the hood**: \n",
    ">\n",
    ">- After installing the sagemaker-training-tookit, we can run it as a command line script just executing `train` in the terminal. \n",
    ">\n",
    ">[Take a look at the setup.py of sagemaker-training-toolkit here.](https://github.com/aws/sagemaker-training-toolkit/blob/master/setup.py) \n",
    ">\n",
    ">(see the last lines of setup.py &rarr; `entry_points={\"console_scripts\": : [\"train=(...)}`)\n",
    ">\n",
    ">- When executing `train` the toolkit will run the specified script and look for this environment variable `SAGEMAKER_TRAINING_MODULE` above. \n",
    ">\n",
    ">In another words:\n",
    ">\n",
    ">- We define `ENV SAGEMAKER_TRAINING_MODULE custom_lightgbm_framework.training:main`. When we run `train` in bash, the SageMaker Training Toolkit execute our `main()` function defined at [custom_lightgbm_framework.training](../package/src/custom_lightgbm_framework/training.py). \n",
    ">\n",
    ">SageMaker will run our Docker image for training with `docker run <YOUR-IMAGE> train` ([more details here](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-dockerfile.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"training_toolkit\">\n",
    "    <h2>2. What is our custom_framework_training-1.0.0.tar.gz package?</h2>\n",
    "</div>\n",
    "\n",
    "Looking at our Dockerfile above, we see that we didn't install sagemaker-training (the toolkit) with `pip install sagemaker-training`, nor created a custom console script for the command `train`. All of that is configured within our package.\n",
    "\n",
    "The advantage here is that **we can re-use the package and change just the libraries installed in the Docker container**. In the end if we wanted to use another Framework (e.g. [Catboost](https://catboost.ai/)) we would just change the step 6. in the Dockerfile (e.g. `RUN pip install catboost`).\n",
    "\n",
    "Let's see the configurations of our custom Python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ../package/setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This build script looks at the packages under the local src/ path and specifies the dependency on sagemaker-training. As previously explained, the training module containing the `main()` function that will be executed is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ../package/src/custom_lightgbm_framework/training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is that we will use the <strong>entry_point.run()</strong> function of the sagemaker-training-toolkit library to execute the user-provided module.\n",
    "You might want to set additional framework-level configurations (e.g. parameter servers) before calling the user module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"container_training_build\">\n",
    "<h2>3. Build and push the container</h2>\n",
    "</div>\n",
    "\n",
    "We are now ready to build this container and push it to Amazon ECR. This task is executed using a shell script stored in the ../script/ folder. Let's take a look at this script and then execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize ../scripts/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This script does the following:\n",
    "\n",
    "1. runs the **setup.py** to create the training package (a tarball in this case) and copy it under **../docker/code/**\n",
    "2. builds the Docker image and tags it\n",
    "3. logs in to Amazon Elastic Container Registry and creates a repository if there's none\n",
    "4. push the image to our ECR repository\n",
    "\n",
    "The build task requires a few minutes to be executed the first time, then Docker caches build outputs to be reused for the subsequent build operations.\n",
    "\n",
    "**Let's run this script now:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confgurations for our bash script:')\n",
    "print('account ID:', account_id)\n",
    "print('region:', region)\n",
    "print('ECR repository name:', ecr_repository_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ../scripts/build_and_push.sh $account_id $region $ecr_repository_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to ECR in the AWS console](https://console.aws.amazon.com/ecr/home?region=us-east-1) and check if our new repository called `sagemaker-custom-lightgbm` was created and the image was pushed to it.\n",
    "\n",
    "![ecr-repo](./media/ecr-repo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"testing_training\">\n",
    "<h2>4. Training with Amazon SageMaker</h2>\n",
    "</div>\n",
    "\n",
    "Once we have correctly pushed our container to Amazon ECR, we are ready to start training with Amazon SageMaker. As previously explained, we have 2 options to pass this training script to SageMaker: via **Script Mode** or **Framework mode**.\n",
    "\n",
    "*We have to:*\n",
    "\n",
    "a) **upload the data to S3** so that SageMaker knows from where to dowload it and train (defining our S3-based training channels).\n",
    "\n",
    "b) to train using our script:\n",
    "- b.1) **if in Script mode** &rarr; upload training script to S3\n",
    "- b.2) **if in Framework mode** &rarr; extend the sagemaker.estimator.Framework class(the SageMaker Python SDK will do the uploading for us)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Uploading the data to S3\n",
    "Let's **upload our data** from the directory `data` (created in the first notebook) to Amazon S3 and configure SageMaker input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data in S3 for training with SageMaker\n",
    "prefix = 'sagemaker-custom'\n",
    "data_dir = 'data'\n",
    "input_train = sagemaker_session.upload_data('data/train/iris_train.csv', key_prefix=\"{}/{}\".format(prefix, data_dir) )\n",
    "input_test = sagemaker_session.upload_data('data/test/iris_test.csv', key_prefix=\"{}/{}\".format(prefix, data_dir) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.list_s3_files(sagemaker_session.default_bucket(), prefix+'/'+data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = sagemaker.session.s3_input(input_train, content_type='text/csv')\n",
    "test_config = sagemaker.session.s3_input(input_test, content_type='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to the S3 bucket to check how the data was stored.](https://s3.console.aws.amazon.com/s3/home)\n",
    "\n",
    "![iris-data](./media/s3-iris-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.1) Now, for the Script mode container:\n",
    "\n",
    "As discussed, in script mode, SageMaker will load the training script from S3. \n",
    "\n",
    "Let's create a tarball with the training script and upload it to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to create a tarball\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "def create_tar_file(source_files, target=None):\n",
    "    if target:\n",
    "        filename = target\n",
    "    else:\n",
    "        _, filename = tempfile.mkstemp()\n",
    "\n",
    "    with tarfile.open(filename, mode=\"w:gz\") as t:\n",
    "        for sf in source_files:\n",
    "            # Add all files from the directory into the root of the directory structure of the tar\n",
    "            t.add(sf, arcname=os.path.basename(sf))\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tarball with our training script\n",
    "create_tar_file([\"source_dir/train.py\"], \"sourcedir.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the tarball to S3\n",
    "sources = sagemaker_session.upload_data('sourcedir.tar.gz', bucket, prefix + '/code')\n",
    "print('Uploaded tarball to:', sources)\n",
    "! rm sourcedir.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure SageMaker to use our Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print(container_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remember the training script we created before ([in the previous notebook](./0_local_development.ipynb)), [click here to view the script.](./source_dir/train.py)\n",
    "\n",
    "Train in Script mode with SageMaker (use `sagemaker.estimator.Estimator`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "\n",
    "# JSON encode hyperparameters.\n",
    "def json_encode_hyperparameters(hyperparameters):\n",
    "    return {str(k): json.dumps(v) for (k, v) in hyperparameters.items()}\n",
    "\n",
    "hyperparameters = json_encode_hyperparameters({\n",
    "    \"sagemaker_program\": \"train.py\",\n",
    "    \"sagemaker_submit_directory\": sources,\n",
    "    \"num_leaves\": 40,\n",
    "    \"max_depth\": 10,\n",
    "    \"learning_rate\": 0.11,\n",
    "    \"random_state\": 42})\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(container_image_uri,\n",
    "                                    role,\n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='local',\n",
    "                                    #train_instance_type='ml.m5.xlarge',\n",
    "                                    base_job_name=prefix,\n",
    "                                    hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the *sagemaker-training-toolkit library* uses the following reserved hyperparameters to know where the our sources are stored in Amazon S3:\n",
    "- `sagemaker_program`\n",
    "- `sagemaker_submit_directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train': train_config, 'validation': test_config })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Again, as expected the training finished with the final validation loss of 0.138846 and F1 score of 0.94 after choosing the same hyperparameters ([look the previous notebook again if you want](./0_local_development.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.2) Now, for the Framework mode container:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen, in the previous steps we had to upload our code to Amazon S3 and then inject reserved hyperparameters to execute training. However, **with the Framework class in the SageMaker Python SDK, the upload is automated for us.**\n",
    "\n",
    "Let's extend the `sagemaker.estimator.Framework` class from the SageMaker Python SDK. We'll create a class called `MyLightGBMFramework` that inherits from the Framework class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile custom_framework.py\n",
    "from sagemaker.estimator import Framework\n",
    "\n",
    "class MyLightGBMFramework(Framework):\n",
    "    def __init__(\n",
    "        self,\n",
    "        entry_point,\n",
    "        source_dir=None,\n",
    "        hyperparameters=None,\n",
    "        py_version=\"py3\",\n",
    "        framework_version=None,\n",
    "        image_name=None,\n",
    "        distributions=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(MyLightGBMFramework, self).__init__(\n",
    "            entry_point, source_dir, hyperparameters, image_name=image_name, **kwargs\n",
    "        )\n",
    "    \n",
    "    def _configure_distribution(self, distributions):\n",
    "        return\n",
    "    \n",
    "    def create_model(\n",
    "        self,\n",
    "        model_server_workers=None,\n",
    "        role=None,\n",
    "        vpc_config_override=None,\n",
    "        entry_point=None,\n",
    "        source_dir=None,\n",
    "        dependencies=None,\n",
    "        image_name=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our `MyLightGBMFramework` class for training with SageMaker and pass the local script and directory (`entry_point` and `source_dir`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from custom_framework import MyLightGBMFramework\n",
    "\n",
    "framework = MyLightGBMFramework(image_name=container_image_uri,\n",
    "                                role=role,\n",
    "                                entry_point='train.py',\n",
    "                                source_dir='source_dir/',\n",
    "                                train_instance_count=1, \n",
    "                                train_instance_type='local', # we use local mode\n",
    "                                #train_instance_type='ml.m5.xlarge',\n",
    "                                base_job_name=prefix,\n",
    "                                hyperparameters={\"num_leaves\": 40,\n",
    "                                                 \"max_depth\": 10,\n",
    "                                                 \"learning_rate\": 0.11,\n",
    "                                                 \"random_state\": 42}\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework.fit({'train': train_config, 'validation': test_config })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Again, as expected the training finished with the final validation loss of 0.138846 and F1 score of 0.94 after choosing the same hyperparameters ([look the previous notebook again if you want](./0_local_development.ipynb)).\n",
    "\n",
    "> If wanted we could have used a Python script that is stored in a Git repository instead of the local file using [git_config](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Framework) (this could be helpful when we automate and create a ML pipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end of the creating the training container! \n",
    "\n",
    "### What's next? How can we deploy the custom model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to deploy it, we'll receive an error (obviously, since we didn't implement the logic for serving the trained model and inference.\n",
    "\n",
    "Click on the **STOP** button (square button on the top) to stop this test:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there's no implementation of web server nor inference logic, if you deploy the model with the script mode (the `estimator` object above) you'll receive the error: \n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\", line 618, in run\n",
    "    _stream_output(self.process)\n",
    "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\", line 677, in _stream_output\n",
    "    raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n",
    "RuntimeError: Process exited with code: 1\n",
    "\n",
    "(...)\n",
    "\n",
    "RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/tmpjswa7hh1/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.deploy(initial_instance_count=1,\n",
    "                 instance_type='local',\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `framework` object above for the Framework mode, again there will be an error since we didn't implement the [Framework Model class](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.FrameworkModel) (when executing `framwork.deploy(...)` the SageMaker Python SDK first creates a Model object and then uses it to deploy ([more details in the source code](https://github.com/aws/sagemaker-python-sdk/blob/4100bfa8aba871c3b947f88891a7719139b6f394/src/sagemaker/estimator.py#L1377)):\n",
    "\n",
    "```\n",
    "AttributeError: 'NoneType' object has no attribute 'name'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework.deploy(initial_instance_count=1,\n",
    "                 instance_type='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement the final logic for serving and inference!\n",
    "\n",
    "## &rarr; [CLICK HERE TO MOVE ON](../../1_custom_inference/lab/2_inference-container.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a docker container for deploying, hosting and monitoring our LightGBM model\n",
    "\n",
    "Now that we have successfully developed the LightGBM model for training, we can proceed and finish our custom Docker container for **serving the model and provide inference logic**. We'll use the [sagemaker-inference-toolkit](https://github.com/aws/sagemaker-inference-toolkit) library to facilitate this tasks.\n",
    "\n",
    "This part of the workshop is composed of 4 parts:\n",
    "1. <a href=\"#custom_inference_container\">Extend our previous <strong>custom Docker container for serving the model</strong> with SageMaker</a>\n",
    "2. <a href=\"#inference_toolkit\">Create a Python package to configure <strong>SageMaker Inference toolkit</strong></a>\n",
    "3. <a href=\"#container_inference_build\"><strong>Build our final Docker image for training and inferece</strong> and <strong>push</strong> it to Amazon Elastic Container Registry</a>\n",
    "3. <a href=\"#testing_inference\"><strong>Testing the inference locally</strong> with our container using the SageMaker Python SDK</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=custom_inference_container>\n",
    "<h2> 1. Creating the inference container</h2>\n",
    "</div>\n",
    "Again, we start by defining some variables like the current execution role, the ECR repository that we are going to use for pushing the final custom Docker container and the default Amazon S3 bucket to be used by Amazon SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "ecr_repository_name = 'sagemaker-custom-lightgbm'\n",
    "role = get_execution_role()\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print('ecr_repository_name:', ecr_repository_name)\n",
    "print('account_id:',account_id)\n",
    "print('region:',region)\n",
    "print('role:',role)\n",
    "print('bucket:',bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we'll write all the Python modules from this notebook with the `%%writefile` built-in magic commands from IPython.\n",
    "\n",
    "We'll store our code in a few directories:\n",
    "```\n",
    "1_custom_inference/\n",
    "│ \n",
    "├── docker\n",
    "│   └── code\n",
    "│   \n",
    "└── package\n",
    "    ├── src\n",
    "    └── custom_lightgbm_inference\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../package\n",
    "!rm -rf ../docker\n",
    "\n",
    "!mkdir ../docker\n",
    "!mkdir ../docker/code\n",
    "\n",
    "!mkdir ../package\n",
    "!mkdir ../package/src\n",
    "!mkdir ../package/src/custom_lightgbm_inference\n",
    "!touch ../package/src/custom_lightgbm_inference/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write another Dockerfile for building our custom end-to-end LightGBM container for **training and serving**.\n",
    "\n",
    "We'll use the previous container already with the [SageMaker Training Toolkit](https://github.com/aws/sagemaker-training-toolkit) and extend it with [SageMaker Inference Toolkit](https://github.com/aws/sagemaker-inference-toolkit) and with our inference logic.\n",
    "\n",
    "**By serving** you can understand: exposing our model as a webservice that can be called through an API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../docker/Dockerfile\n",
    "# 1. Use with our previous container as base image\n",
    "FROM sagemaker-custom-lightgbm:latest\n",
    "\n",
    "# 2. Defining some variables used at build time to install Python3\n",
    "ARG PYTHON=python3\n",
    "ARG PYTHON_PIP=python3-pip\n",
    "ARG PIP=pip3\n",
    "ARG PYTHON_VERSION=3.6.6\n",
    "\n",
    "# 3. Set a docker label to advertise multi-model support on the container \n",
    "LABEL com.amazonaws.sagemaker.capabilities.multi-models=false\n",
    "# Set a docker label to enable container to use SAGEMAKER_BIND_TO_PORT environment variable if present\n",
    "LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
    "\n",
    "# 4. Instal libraries for the sagemaker-inference and multi-model-server libraries\n",
    "RUN apt-get update -y && apt-get -y install --no-install-recommends default-jdk\n",
    "RUN rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# 5. Copy our package to the WORKDIR\n",
    "COPY code/custom_lightgbm_inference-0.1.0.tar.gz /custom_lightgbm_inference-0.1.0.tar.gz\n",
    "        \n",
    "# 6. Installing our custom package for inference\n",
    "RUN ${PIP} install --no-cache --upgrade \\\n",
    "        /custom_lightgbm_inference-0.1.0.tar.gz && \\\n",
    "    rm /custom_lightgbm_inference-0.1.0.tar.gz\n",
    "\n",
    "# 7. Set SageMaker serving environment variables\n",
    "ENV SM_MODEL_DIR /opt/ml/model\n",
    "ENV CODE_DIR /opt/ml/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This time, for simplicity, we won't provide logic to be injected inside the container for inference. We'll stick a a pre-defined default inference logic (we don't set environment variables like `ENV SAGEMAKER_SERVING_MODULE sagemaker_custom.serving:main`). However, if desired, we could do something similar to [what was shown in the training lab](../../0_custom_train/lab/1_training-container.ipynb) to provide user-defined logic for inference.\n",
    ">\n",
    "> SageMaker will run our Docker image for training with `docker run <YOUR-IMAGE> serve` ([more details here](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html)). After installing our custom Python package we can run it as a command line script when executing `serve` in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"inference_toolkit\">\n",
    "    <h2>2. Creating our Python Package with the SageMaker Inference Toolkit</h2>\n",
    "</div>\n",
    "\n",
    "The **Inference Handler** is how we use the SageMaker Inference Toolkit to encapsulate our code and expose it as a SageMaker container. We create a `handler.py` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../package/src/custom_lightgbm_inference/handler.py\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from sagemaker_inference.default_inference_handler import DefaultInferenceHandler\n",
    "from sagemaker_inference.default_handler_service import DefaultHandlerService\n",
    "from sagemaker_inference import content_types, errors, transformer, encoder, decoder\n",
    "\n",
    "class HandlerService(DefaultHandlerService, DefaultInferenceHandler):\n",
    "    def __init__(self):\n",
    "        op = transformer.Transformer(default_inference_handler=self)\n",
    "        super(HandlerService, self).__init__(transformer=op)\n",
    "    \n",
    "    ## Loads the model from the disk\n",
    "    def default_model_fn(self, model_dir):\n",
    "        model_filename = os.path.join(model_dir, \"model.joblib\")\n",
    "        return joblib.load(model_filename)\n",
    "    \n",
    "    ## Parse and check the format of the input data\n",
    "    def default_input_fn(self, input_data, content_type):\n",
    "        if content_type != \"text/csv\":\n",
    "            raise Exception(\"Invalid content-type: %s\" % content_type)\n",
    "        return decoder.decode(input_data, content_type).reshape(1,-1)\n",
    "    \n",
    "    ## Run our model and do the prediction\n",
    "    def default_predict_fn(self, payload, model):\n",
    "        return model.predict( payload ).tolist()\n",
    "    \n",
    "    ## Gets the prediction output and format it to be returned to the user\n",
    "    def default_output_fn(self, prediction, accept):\n",
    "        if accept != \"text/csv\":\n",
    "            raise Exception(\"Invalid accept: %s\" % accept)\n",
    "        return encoder.encode(prediction, accept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the entrypoint of our Python package. The `main()` function will be ran as a console script later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../package/src/custom_lightgbm_inference/my_serving.py\n",
    "from sagemaker_inference import model_server\n",
    "from custom_lightgbm_inference import handler\n",
    "\n",
    "HANDLER_SERVICE = handler.__name__\n",
    "\n",
    "def main():\n",
    "    print('Running handler service:', HANDLER_SERVICE)\n",
    "    model_server.start_model_server(handler_service=HANDLER_SERVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define our `setup.py` file to create our custom inference package with setuptools.\n",
    "\n",
    "We setup a entry point so that by running `serve` in the terminal our model server will start (we run the main function above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../package/setup.py\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from os.path import basename\n",
    "from os.path import splitext\n",
    "\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "setup(\n",
    "    name='custom_lightgbm_inference',\n",
    "    version='0.1.0',\n",
    "    description='Custom container serving package for SageMaker.',\n",
    "    keywords=\"custom container serving package SageMaker\",\n",
    "\n",
    "    packages=find_packages(where='src'),\n",
    "    package_dir={'': 'src'},\n",
    "    py_modules=[splitext(basename(path))[0] for path in glob('src/*.py')],\n",
    "    \n",
    "    install_requires=[\n",
    "        'sagemaker-inference==1.3.0',\n",
    "        'multi-model-server==1.1.2'\n",
    "    ],\n",
    "    entry_points={\"console_scripts\": [\"serve=custom_lightgbm_inference.my_serving:main\"]},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"container_inference_build\">\n",
    "<h2>3. Build and push the container</h2>\n",
    "</div>\n",
    "\n",
    "We are now ready to build this container and push it to Amazon ECR. This task is executed using a shell script stored in the `../script/` folder like before. Let's take a look at this script and then execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize ../scripts/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Let's run this script now:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confgurations for our bash script:')\n",
    "print('account ID:', account_id)\n",
    "print('region:', region)\n",
    "print('ECR repository name:', ecr_repository_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ../scripts/build_and_push.sh $account_id $region $ecr_repository_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to ECR in the AWS console](https://console.aws.amazon.com/ecr/home?region=us-east-2) and check if our `sagemaker-custom-lightgbm` repository has the updated Docker image.\n",
    "\n",
    "![ecr-repo-updated](./media/ecr-repo-updated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"testing_inference\">\n",
    "<h2>4. End-to-end test with Amazon SageMaker</h2>\n",
    "</div>\n",
    "\n",
    "Once we have correctly pushed our container to Amazon ECR, we are now ready for our final test using with Amazon SageMaker. \n",
    "\n",
    "As previously explained, we have 2 options use our LightGBM Docker image: via **Script Mode** (using the `sagemaker.estimator.Estimator` class) or **Framework mode** (using the `sagemaker.estimator.Framework` class).\n",
    "\n",
    "For simplicity, we will use the **Script Mode** (to use the Framework mode we would have to implement the [Framework Model class](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.FrameworkModel)).\n",
    "\n",
    "*With  the SageMaker Python SDK we will:*\n",
    "\n",
    "a) **train a LightGBM model** using the [sagemaker.estimator.Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Estimator) class and calling the `fit(...)` method\n",
    "\n",
    "b) **deploy our LightGBM model** calling the `deploy(...)` method of the estimator and receiving a [Predictor object](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor)\n",
    "\n",
    "c) **send requests to our deployed LightGBM model** calling the `predict(...)` method of the predictor object\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print(container_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = f's3://sagemaker-{region}-{account_id}/sagemaker-custom/data/iris_train.csv'\n",
    "test_config = f's3://sagemaker-{region}-{account_id}/sagemaker-custom/data/iris_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = f's3://sagemaker-{region}-{account_id}/sagemaker-custom/code/sourcedir.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) train a LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "\n",
    "# JSON encode hyperparameters.\n",
    "def json_encode_hyperparameters(hyperparameters):\n",
    "    return {str(k): json.dumps(v) for (k, v) in hyperparameters.items()}\n",
    "\n",
    "hyperparameters = json_encode_hyperparameters({\n",
    "    \"sagemaker_program\": \"train.py\",\n",
    "    \"sagemaker_submit_directory\": sources,\n",
    "    \"num_leaves\": 40,\n",
    "    \"max_depth\": 10,\n",
    "    \"learning_rate\": 0.11,\n",
    "    \"random_state\": 42})\n",
    "\n",
    "prefix = 'sagemaker-custom-final'\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(container_image_uri,\n",
    "                                    role,\n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='local',\n",
    "                                    #train_instance_type='ml.m5.xlarge',\n",
    "                                    base_job_name=prefix,\n",
    "                                    hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train': train_config, 'validation': test_config })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) deploy our LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "                 instance_type='local',\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) send requests to our deployed LightGBM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sagemaker.predictor import csv_serializer, csv_deserializer\n",
    "\n",
    "# configure the predictor to do everything for us\n",
    "predictor.content_type = 'text/csv'\n",
    "predictor.accept = 'text/csv'\n",
    "predictor.serializer = csv_serializer\n",
    "predictor.deserializer = None\n",
    "\n",
    "# load the testing data from the validation csv\n",
    "validation = pd.read_csv('../../0_custom_train/lab/data/test/iris_test.csv', header=None)\n",
    "idx = random.randint(0,len(validation)-5)\n",
    "req = validation.iloc[idx:idx+5].values\n",
    "\n",
    "# cut a sample with 5 lines from our dataset and then split the label from the features.\n",
    "X = req[:,0:-1].tolist()\n",
    "y = req[:,-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the local endpoint\n",
    "for features,label in zip(X,y):\n",
    "    prediction = float(predictor.predict(features).decode('utf-8').strip())\n",
    "\n",
    "    # compare the results\n",
    "    print(\"\\nRESULT: {} == {} ? {}\\n\".format( label, prediction, label == prediction ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The end of the custom Docker container development! \n",
    "\n",
    "If you'd like to dive even deeper into how to create your own custom Docker containers for SageMaker, a nice place to start is looking at the **open source implementations of a few SageMaker containers**:\n",
    "\n",
    "- **SageMaker Scikit-Learn container**: https://github.com/aws/sagemaker-scikit-learn-container\n",
    "\n",
    "- **SageMaker XGBoost container**: https://github.com/aws/sagemaker-xgboost-container\n",
    "\n",
    "\n",
    "\n",
    "Note that these SageMaker containers use similar similar logic with Python package console scripts and similar libraries to what we used here:\n",
    "- multi-model-server\n",
    "- sagemaker-inference\n",
    "- sagemaker-training\n",
    "\n",
    "(Take a look at the [setup.py](https://github.com/aws/sagemaker-scikit-learn-container/blob/master/setup.py) and [requirement.txt](https://github.com/aws/sagemaker-scikit-learn-container/blob/master/requirements.txt) files for more details).\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "### Let's automate everything in a ML pipeline!\n",
    "\n",
    "## &rarr; [CLICK HERE TO MOVE ON](../../2_pipeline/3_ml-pipeline.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker_local",
   "language": "python",
   "name": "sagemaker_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

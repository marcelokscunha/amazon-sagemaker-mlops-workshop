{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing our LightGBM model locally\n",
    "\n",
    "Before creating our LightGBM container for SageMaker, let's create a simple model and test it locally.\n",
    "\n",
    "Let's install LightGBM to the SageMaker Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm==2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this experiment is a toy dataset called Iris (http://archive.ics.uci.edu/ml/datasets/iris). The dataset contains 3 classes of 50 instances each, where each class refers to a type of iris plant and the goal is to classify the correct species based on features like sepal and petal width and length. \n",
    "\n",
    "<img src='./media/iris.jpg' alt='iris'  class=\"center\">\n",
    "\n",
    "The clallenge itself is very basic, so you can focus on the mechanics and the features of this automated environment later.\n",
    "\n",
    "As a requirement, suppose our **F1 score must be greater than 90%** in order for our model to go to production.\n",
    "\n",
    "Feel free to develop your own **LightGBM model** (take a look in the [docs for the parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html) and [the Python APIs](https://lightgbm.readthedocs.io/en/latest/Python-API.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "dataset = np.insert(iris.data, 0, iris.target,axis=1)\n",
    "\n",
    "df = pd.DataFrame(data=dataset, columns=['iris_id'] + iris.feature_names)\n",
    "## We'll also save the dataset, with header, give we'll need to create a baseline for the monitoring\n",
    "df['species'] = df['iris_id'].map(lambda x: 'setosa' if x == 0 else 'versicolor' if x == 1 else 'virginica')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's use the [LightGBM Scikit Learn's API](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random number seed to make it reproducible\n",
    "gbm = lgb.LGBMClassifier(objective='multiclass',\n",
    "                        num_class=len(np.unique(y)),\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.set_params(num_leaves=40,\n",
    "              max_depth=10,\n",
    "              learning_rate=0.11,\n",
    "              random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_names='[validation_softmax]',\n",
    "        eval_metric='softmax',\n",
    "        early_stopping_rounds=5,\n",
    "        verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = f1_score(y_test,y_pred,labels=[0.0,1.0,2.0],average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look's like our F1 score is good enough and our code is working!\n",
    "\n",
    "We create a directory called `models` to save the trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gbm, os.path.join('models', 'nb_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the loaded model\n",
    "loaded_gbm = joblib.load('models/nb_model.joblib')\n",
    "\n",
    "y_loaded_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "\n",
    "f1_score(y_test,y_loaded_pred,labels=[0.0,1.0,2.0],average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's turn the code cells above into a Python script. This way we'll be able to automate the training and also use our custom training script in SageMaker.\n",
    "\n",
    "The idea is that we can **pass the hyperparameters and also the data location into our script**. After training, the script **will save our model in the specified directory**. In addition, the script can load a specific model from a directory, with a function called `model_fn`. We could run the script multiple times with different configurations if wanted.\n",
    "\n",
    "First, we save the train and test datasets to a local folder called `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory and write csv\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "os.makedirs('./data/raw', exist_ok=True)\n",
    "os.makedirs('./data/train', exist_ok=True)\n",
    "os.makedirs('./data/test', exist_ok=True)\n",
    "os.makedirs('./data/test_no_label', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data_raw = np.concatenate((X, np.expand_dims(y, axis=1)), axis=1)\n",
    "np.savetxt('./data/raw/iris.csv', np_data_raw, delimiter=',', fmt='%1.1f, %1.1f, %1.1f, %1.1f, %1.0f')\n",
    "\n",
    "np_data_train = np.concatenate((X_train, np.expand_dims(y_train, axis=1)), axis=1)\n",
    "np.savetxt('./data/train/iris_train.csv', np_data_train, delimiter=',', fmt='%1.1f, %1.1f, %1.1f, %1.1f, %1.0f')\n",
    "\n",
    "np_data_test = np.concatenate((X_test, np.expand_dims(y_test, axis=1)), axis=1)\n",
    "np.savetxt('./data/test/iris_test.csv', np_data_test, delimiter=',', fmt='%1.1f, %1.1f, %1.1f, %1.1f, %1.0f')\n",
    "np.savetxt('./data/test_no_label/iris_test_no_label.csv', X_test, delimiter=',', fmt='%1.1f, %1.1f, %1.1f, %1.1f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create the training script `train.py` and save it in a local directory called `source_dir`. We save the trained model in the `models` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./source_dir', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile source_dir/train.py\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import joblib\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def _get_data(path):\n",
    "    input_files = [ os.path.join(path, file) for file in os.listdir(path) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "\n",
    "    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n",
    "    data = pd.concat(raw_data)\n",
    "    X=data.iloc[:,:4]\n",
    "    y=data.iloc[:,4]\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "\n",
    "def train(args):\n",
    "    '''\n",
    "    Main function for initializing SageMaker training in the hosted infrastructure.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args: the parsed input arguments of the script. The objects assigned as attributes of the namespace. It's the populated namespace.\n",
    "    \n",
    "    See: https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser.parse_args\n",
    "    '''\n",
    "\n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    logger.info('Loading the data...')\n",
    "    \n",
    "    X_train, y_train = _get_data(args.train)\n",
    "    X_test, y_test = _get_data(args.test)\n",
    "    \n",
    "    logger.info(f'Train data with shape: X={X_train.shape} y={y_train.shape}')\n",
    "    logger.info(f'Validation data with shape: X={X_test.shape} y={y_test.shape}')\n",
    "\n",
    "    logger.info('Starting training...')\n",
    "    gbm = lgb.LGBMClassifier(objective='multiclass',\n",
    "                            num_class=len(np.unique(y_train)))\n",
    "    \n",
    "    hyperparams = {a_key: a_value for a_key, a_value in vars(args).items() if (a_value!=None and a_key not in ['model_dir', 'train', 'test'])}\n",
    "    print('hyperparameters:', hyperparams)\n",
    "    \n",
    "    gbm.set_params(**hyperparams)\n",
    "    \n",
    "    logger.info(f'Using configuration:\\n{gbm}')\n",
    "    gbm.fit(X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_names='[validation_softmax]',\n",
    "            eval_metric='softmax',\n",
    "            early_stopping_rounds=5,\n",
    "            verbose=5)\n",
    "\n",
    "    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "\n",
    "    score = f1_score(y_test,y_pred,labels=[0.0,1.0,2.0],average='micro')\n",
    "\n",
    "    # generate evaluation metrics\n",
    "    logger.info(f'[F1 score] {score}')\n",
    "                                                              \n",
    "    save_model(gbm, args.model_dir)\n",
    "                                                              \n",
    "def save_model(model, model_dir):\n",
    "    '''\n",
    "    Function for saving the model in the expected directory for SageMaker.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: a Scikit-Learn estimator\n",
    "    model_dir: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting. (this should be the default SageMaker environment variables)\n",
    "    '''\n",
    "    logger.info(f\"Saving the model in directory '{model_dir}'\")\n",
    "                                                              \n",
    "    # Print the coefficients of the trained classifier, and save the coefficients\n",
    "    joblib.dump(model, os.path.join(model_dir, \"model.joblib\"))\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "    \n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    estimator = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return estimator\n",
    "\n",
    "\n",
    "# Main script entry for SageMaker to run when initializing training\n",
    "                                                              \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters (if not specified, default to LightGBM')\n",
    "    parser.add_argument('--num_leaves', type=int, default=None)\n",
    "    parser.add_argument('--max_depth', type=int, default=None)\n",
    "    parser.add_argument('--learning_rate', type=float, default=None)\n",
    "    parser.add_argument('--random_state', type=int, default=None) \n",
    "    \n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "#     print(args)\n",
    "                                                              \n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the SageMaker enviroment, the default directories will be:\n",
    "%env SM_MODEL_DIR=\"/opt/ml/model\"\n",
    "%env SM_CHANNEL_TRAIN=\"/opt/ml/input/data/train\"\n",
    "%env SM_CHANNEL_VALIDATION=\"/opt/ml/input/data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run our script locally we will overwrite the defaults (passing other local directories to load data and save models)\n",
    "# We will run with LGBM's defaults (no hyperparams defined):\n",
    "!python source_dir/train.py --model-dir models --train data/train --test data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will run will run with LGBM's other hyperparameters:\n",
    "# (We expect final validation loss of 0.138846 and F1 score of 0.94)\n",
    "!python source_dir/train.py --num_leaves 40 --max_depth 10 --learning_rate 0.11 --random_state 42 --model-dir models --train data/train --test data/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look's like our script is working as expected!\n",
    "\n",
    "We check if the models was saved correctly in the `models` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_loaded = joblib.load('models/model.joblib')\n",
    "gbm_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end of the local development! \n",
    "\n",
    "## Now, we will develop our custom LightGBM container\n",
    "\n",
    "## &rarr; [CLICK HERE TO MOVE ON](./1_training-container.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
